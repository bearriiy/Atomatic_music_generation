
import os
import pickle
import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from utils import MusicDataset, MusicGeneratorUtils
from model import TransformerMusicModel
import sys
import os
# ==========================
# å…¨å±€é…ç½®å‚æ•°ï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰
# ==========================

# cd backend; python app.py

# --- è¿è¡Œæ¨¡å¼é€‰æ‹© ---
# å¯é€‰å€¼: "conditional" (æƒ…ç»ªç”Ÿæˆ), "inpainting" (æ—‹å¾‹è¡¥å…¨)
mode = "conditional"  # <<< ä¿®æ”¹è¿™é‡Œåˆ‡æ¢æ¨¡å¼

# --- è·¯å¾„é…ç½®ï¼ˆæ ¹æ® mode åŠ¨æ€è®¾ç½®ï¼‰---
if mode == "conditional":
    midi_dir = "midi_songs"          # å­ç›®å½•ä¸ºæƒ…ç»ªç±»åˆ«ï¼ˆå¦‚ Q1/, Q2/...ï¼‰
    model_name = "conditional_music_model"
    data_dir = "data"

elif mode == "inpainting":
    midi_dir = "fill_songs"     # å•ä¸€æ–‡ä»¶å¤¹ï¼Œå­˜æ”¾ç”¨äºæ—‹å¾‹è¡¥å…¨è®­ç»ƒçš„MIDI
    model_name = "inpainting_music_model"
    data_dir = "fill_data"
else:
    raise ValueError("mode å¿…é¡»æ˜¯ 'conditional' æˆ– 'inpainting'")

models_dir = "models"
output_dir = "output"

model_filename = f"{model_name}.pth"

# --- æ•°æ®å¤„ç†å‚æ•° ---
sequence_length = 100

# --- æ¨¡å‹æ¶æ„å‚æ•° ---
vocab_embed_dim = 128
num_heads = 8
num_layers = 3
dropout_rate = 0.3
feedforward_dim = 512

# --- è®­ç»ƒå‚æ•° ---
batch_size = 64
learning_rate = 0.001
weight_decay = 1e-5
clip_grad_norm = 1.0
epochs = 10

# --- ç”Ÿæˆå‚æ•°ï¼ˆä»… conditional æ¨¡å¼ä½¿ç”¨ï¼‰---
default_num_notes = 100
generation_temperature = {
    "Q1": 1.2,
    "Q2": 1.3,
    "Q3": 0.8,
    "Q4": 0.7,
    "GiantMIDI-Piano": 1.5,  # æ›´é«˜æ¸©åº¦ â†’ æ›´å¤§éšæœºæ€§
}
tempo_range = {
    "Q1": (130, 160),
    "Q2": (140, 180),
    "Q3": (50, 80),
    "Q4": (70, 100),
    "GiantMIDI-Piano": (40, 200),  # è¦†ç›–ææ…¢åˆ°æå¿«ï¼Œå¢å¼ºæƒ…ç»ªè·¨åº¦
}
pitch_range = {
    "Q1": (65, 88),
    "Q2": (60, 90),
    "Q3": (40, 65),
    "Q4": (55, 75),
    "GiantMIDI-Piano": (21, 108),  # MIDI å…¨é”®ç›˜èŒƒå›´ï¼ˆA0=21 åˆ° C8=108ï¼‰
}

# --------------------------
# éŸ³ä¹ç”Ÿæˆå™¨æ ¸å¿ƒç±»
# --------------------------
class MusicGenerator:
    def __init__(self):
        self.mode = mode
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–å·¥å…·ç±»
        if self.mode == "conditional":
            self.utils = MusicGeneratorUtils(
                midi_dir=midi_dir,
                data_dir=data_dir,
                models_dir=models_dir,
                output_dir=output_dir,
                sequence_length=sequence_length,
                generation_temperature=generation_temperature,
                tempo_range=tempo_range,
                pitch_range=pitch_range
            )
        else:  # inpainting
            self.utils = MusicGeneratorUtils(
                midi_dir=midi_dir,
                data_dir=data_dir,
                models_dir=models_dir,
                output_dir=output_dir,
                sequence_length=sequence_length,
                generation_temperature=generation_temperature,
                tempo_range=tempo_range,
                pitch_range=pitch_range
            )

        print(f"ğŸµ ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.mode == "conditional":
            if self.utils.emotion_to_id:
                print(f"ğŸ­ æ£€æµ‹åˆ°æƒ…ç»ªç±»åˆ«: {list(self.utils.emotion_to_id.keys())}")
            else:
                print("âš ï¸  æœªåœ¨ midi_songs/ ä¸‹æ‰¾åˆ°ä»»ä½•æƒ…ç»ªå­ç›®å½•ï¼")
        else:
            print("ğŸ¹ è¿›å…¥æ—‹å¾‹è¡¥å…¨æ¨¡å¼ï¼ˆinpaintingï¼‰")

    def create_model(self, vocab_size, num_emotions=1):
        """åˆ›å»ºTransformerç¥ç»ç½‘ç»œ"""
        print("ğŸ§  æ­£åœ¨åˆ›å»ºTransformerç¥ç»ç½‘ç»œ...")
        model = TransformerMusicModel(
            vocab_size=vocab_size,
            num_emotions=num_emotions,
            d_model=vocab_embed_dim,
            nhead=num_heads,
            num_layers=num_layers,
            dropout=dropout_rate,
            feedforward_dim=feedforward_dim
        ).to(self.device)
        return model

    def train_model(self, src_sequences, tgt_sequences, emotion_ids, note_to_int, pitchnames):
        vocab_size = len(pitchnames)
        num_emotions = len(set(emotion_ids)) if self.mode == "conditional" else 1

        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: vocab_size={vocab_size}, num_emotions={num_emotions}")

        src_sequences = torch.LongTensor(src_sequences)
        tgt_sequences = torch.LongTensor(tgt_sequences)
        emotion_ids = torch.LongTensor(emotion_ids)

        dataset = MusicDataset(src_sequences, tgt_sequences, emotion_ids)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        model = self.create_model(vocab_size, num_emotions)
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

        best_loss = float('inf')
        best_model_path = os.path.join(models_dir, model_filename)

        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for src, tgt, emo in dataloader:
                src, tgt, emo = src.to(self.device), tgt.to(self.device), emo.to(self.device)
                output = model(src, tgt, emo)
                loss = criterion(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)
                optimizer.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            print(f"ğŸ“Š Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

            if avg_loss < best_loss:
                best_loss = avg_loss
                torch.save({
                    "model_state_dict": model.state_dict(),
                    "vocab_size": vocab_size,
                    "num_emotions": num_emotions,
                    "note_to_int": note_to_int,
                    "pitchnames": pitchnames,
                    "emotion_to_id": getattr(self.utils, 'emotion_to_id', None)
                }, best_model_path)
                print(f"ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (loss={avg_loss:.4f}): {best_model_path}")

        return model

    def load_model(self, model_path=None):
        if model_path is None:
            model_path = os.path.join(models_dir, model_filename)
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device)
        model = TransformerMusicModel(
            vocab_size=checkpoint["vocab_size"],
            num_emotions=checkpoint["num_emotions"],
            d_model=vocab_embed_dim,
            nhead=num_heads,
            num_layers=num_layers,
            dropout=dropout_rate,
            feedforward_dim=feedforward_dim
        ).to(self.device)
        model.load_state_dict(checkpoint["model_state_dict"])
        self.note_to_int = checkpoint["note_to_int"]
        self.int_to_note = {v: k for k, v in self.note_to_int.items()}
        self.emotion_to_id = checkpoint.get("emotion_to_id", None)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        return model

    def generate_conditional(self, emotion="Q1", num_notes=default_num_notes, output_file="generated.mid"):
        """æƒ…ç»ªæ§åˆ¶ç”Ÿæˆ"""
        try:
            model = self.load_model()
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return None

        if emotion not in self.emotion_to_id:
            print(f"âŒ ä¸æ”¯æŒçš„æƒ…ç»ª: {emotion}ï¼Œå¯ç”¨æƒ…ç»ª: {list(self.emotion_to_id.keys())}")
            return None

        emotion_id = self.emotion_to_id[emotion]

        notes_data_path = os.path.join(data_dir, "notes_with_emotion.pkl")
        if not os.path.exists(notes_data_path):
            print("âŒ æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
            return None

        with open(notes_data_path, "rb") as f:
            notes, _, _ = pickle.load(f)

        if len(notes) < sequence_length:
            print(f"âŒ éŸ³ç¬¦æ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {sequence_length} ä¸ªéŸ³ç¬¦")
            return None

        start_idx = np.random.randint(0, len(notes) - sequence_length)
        start_notes = notes[start_idx:start_idx + sequence_length]

        try:
            start_int = [self.note_to_int[note] for note in start_notes]
        except KeyError as e:
            print(f"âŒ èµ·å§‹åºåˆ—åŒ…å«æœªçŸ¥éŸ³ç¬¦: {e}")
            return None

        start_tensor = torch.tensor([start_int], dtype=torch.long).to(self.device)
        temperature = generation_temperature[emotion]
        generated_int = model.generate(start_tensor, emotion_id, max_len=num_notes, temperature=temperature)
        generated_notes = [self.int_to_note.get(i, "60") for i in generated_int]

        return self.utils.create_midi_from_notes(generated_notes, output_file, emotion)

    def complete_melody(self, input_midi_path, output_file="completed.mid", num_completion_notes=100):
        """æ—‹å¾‹è¡¥å…¨ï¼šè¯»å–è¾“å…¥MIDIï¼Œæå–å‰Nä¸ªéŸ³ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¡¥å…¨åç»­"""
        try:
            model = self.load_model()
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return None

        # è¯»å–è¾“å…¥ MIDIï¼ˆå‚è€ƒutilsä¸­çš„æ–¹æ³•ï¼‰
        try:
            from music21 import converter, instrument, note, chord
            midi = converter.parse(input_midi_path)
            parts = instrument.partitionByInstrument(midi)
            notes_to_parse = parts.parts[0].recurse() if parts else midi.flat.notes
            
            notes = []
            for element in notes_to_parse:
                if isinstance(element, note.Note):
                    notes.append(str(element.pitch))
                elif isinstance(element, chord.Chord):
                    notes.append('.'.join(str(n) for n in element.normalOrder))
            
            if not notes:
                print("âŒ è¾“å…¥MIDIä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆéŸ³ç¬¦")
                return None
            print(f"ğŸ“¥ ä»è¾“å…¥MIDIä¸­æå–äº† {len(notes)} ä¸ªéŸ³ç¬¦")
        except Exception as e:
            print(f"âŒ æ— æ³•è§£æè¾“å…¥MIDIæ–‡ä»¶: {e}")
            return None

        # å¤„ç†éŸ³ç¬¦åºåˆ—é•¿åº¦
        if len(notes) < sequence_length:
            print(f"âš ï¸ è¾“å…¥éŸ³ç¬¦ä¸è¶³ {sequence_length}ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……")
            # ä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­çš„å¸¸è§éŸ³ç¬¦è¿›è¡Œå¡«å……ï¼Œè€Œä¸æ˜¯ç®€å•é‡å¤
            if hasattr(self, 'note_to_int'):
                common_notes = list(self.note_to_int.keys())
                if common_notes:
                    # ä»å¸¸è§éŸ³ç¬¦ä¸­éšæœºé€‰æ‹©å¡«å……
                    fill_notes = np.random.choice(common_notes, sequence_length - len(notes))
                    notes.extend(fill_notes)
                else:
                    # å¦‚æœæ²¡æœ‰è¯æ±‡è¡¨ä¿¡æ¯ï¼Œä½¿ç”¨Cå¤§è°ƒéŸ³é˜¶å¡«å……
                    c_major_scale = ['60', '62', '64', '65', '67', '69', '71', '72']  # C4 to C5
                    while len(notes) < sequence_length:
                        notes.append(np.random.choice(c_major_scale))
            else:
                print("âŒ æ— æ³•è·å–è¯æ±‡è¡¨ä¿¡æ¯è¿›è¡Œæ™ºèƒ½å¡«å……")
                return None
        else:
            notes = notes[:sequence_length]

        # è¿‡æ»¤æœªçŸ¥éŸ³ç¬¦ï¼Œä½¿ç”¨æœ€æ¥è¿‘çš„è®­ç»ƒé›†ä¸­å­˜åœ¨çš„éŸ³ç¬¦
        start_notes = []
        for note in notes:
            if note in self.note_to_int:
                start_notes.append(note)
            else:
                # æ‰¾åˆ°æœ€æ¥è¿‘çš„è®­ç»ƒé›†ä¸­å­˜åœ¨çš„éŸ³ç¬¦
                try:
                    # å¤„ç†å’Œå¼¦
                    if '.' in note:
                        # å¯¹äºå’Œå¼¦ï¼Œå°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„å’Œå¼¦è¡¨ç¤º
                        closest_chord = min(self.note_to_int.keys(), 
                                          key=lambda x: len(set(note.split('.')) & set(x.split('.'))))
                        start_notes.append(closest_chord)
                        print(f"ğŸ”§ å°†æœªçŸ¥å’Œå¼¦ {note} æ›¿æ¢ä¸º {closest_chord}")
                    else:
                        # å¯¹äºå•éŸ³ç¬¦
                        note_pitch = int(note)
                        closest_note = min(self.note_to_int.keys(), 
                                         key=lambda x: abs(int(x) - note_pitch) if '.' not in x else float('inf'))
                        start_notes.append(closest_note)
                        print(f"ğŸ”§ å°†æœªçŸ¥éŸ³ç¬¦ {note} æ›¿æ¢ä¸º {closest_note}")
                except (ValueError, TypeError):
                    # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œä½¿ç”¨é»˜è®¤éŸ³ç¬¦
                    start_notes.append("60")
                    print(f"ğŸ”§ å°†æ— æ•ˆéŸ³ç¬¦ {note} æ›¿æ¢ä¸º 60")
        
        # å‚è€ƒgenerate_conditionalæ–¹æ³•è¿›è¡Œæ—‹å¾‹è¡¥å…¨
        try:
            # å°†éŸ³ç¬¦è½¬æ¢ä¸ºæ•´æ•°è¡¨ç¤º
            start_int = [self.note_to_int[note] for note in start_notes]
            
            # åˆ›å»ºè¾“å…¥å¼ é‡
            start_tensor = torch.tensor([start_int], dtype=torch.long).to(self.device)
            
            # ç¡®å®šæ¸©åº¦å‚æ•°ï¼ˆå¯¹äºè¡¥å…¨ä»»åŠ¡ï¼Œä½¿ç”¨é€‚å½“çš„æ¸©åº¦ï¼‰
            temperature = generation_temperature.get("GiantMIDI-Piano", 1.0)
            
            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆåç»­éŸ³ç¬¦
            generated_int = model.generate(start_tensor, 0, max_len=num_completion_notes, temperature=temperature)
            
            # å°†ç”Ÿæˆçš„æ•´æ•°è½¬æ¢å›éŸ³ç¬¦
            generated_notes = [self.int_to_note.get(i, "60") for i in generated_int]
            
            # åˆå¹¶å¤„ç†åçš„ç§å­éŸ³ç¬¦å’Œç”Ÿæˆçš„éŸ³ç¬¦
            complete_notes = start_notes + generated_notes
            
            # ç”ŸæˆMIDIæ–‡ä»¶ï¼ˆä½¿ç”¨GiantMIDI-Pianoä½œä¸ºé»˜è®¤æƒ…ç»ªé…ç½®ï¼‰
            return self.utils.create_midi_from_notes(complete_notes, output_file, "GiantMIDI-Piano")
        except KeyError as e:
            print(f"âŒ éŸ³ç¬¦è½¬æ¢é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ æ—‹å¾‹ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None

    def train_from_scratch(self):
        print("ğŸ”„ å¼€å§‹ä»å¤´è®­ç»ƒæ¨¡å‹...")

        if self.mode == "conditional":
            notes, emotion_labels, emotion_to_id = self.utils.get_notes_with_emotion()
            if not notes:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„éŸ³ç¬¦æ•°æ®")
                return None
            src_seq, tgt_seq, emo_ids, note_to_int, pitchnames = self.utils.prepare_sequences(notes, emotion_labels)
        else:  # inpainting
            notes, _, _ = self.utils.get_notes_with_emotion()  # å¿½ç•¥æƒ…ç»ª
            if not notes:
                print("âŒ midi_inpainting/ ç›®å½•ä¸ºç©º")
                return None
            # æ„é€  dummy emotion_idsï¼ˆå…¨0ï¼‰
            emotion_labels = [0] * len(notes)
            src_seq, tgt_seq, emo_ids, note_to_int, pitchnames = self.utils.prepare_sequences(notes, emotion_labels)

        if src_seq is None:
            print("âŒ åºåˆ—å‡†å¤‡å¤±è´¥")
            return None

        model = self.train_model(src_seq, tgt_seq, emo_ids, note_to_int, pitchnames)

        # ä¿å­˜æ˜ å°„
        os.makedirs(data_dir, exist_ok=True)
        with open(os.path.join(data_dir, "note_to_int.pkl"), "wb") as f:
            pickle.dump(note_to_int, f)
        if self.mode == "conditional":
            with open(os.path.join(data_dir, "emotion_to_id.pkl"), "wb") as f:
                pickle.dump(emotion_to_id, f)

        print("âœ… è®­ç»ƒå®Œæˆ")
        return model


def main():
    generator = MusicGenerator()
    model_path = os.path.join(models_dir, model_filename)

    if os.path.exists(model_path):
        print("ğŸµ å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹")
        if generator.mode == "conditional":
            emotions = list(generator.utils.emotion_to_id.keys())
            for emotion in emotions:
                output_file = os.path.join(output_dir, f"demo_output_{emotion}.mid")
                result = generator.generate_conditional(
                    emotion=emotion,
                    num_notes=default_num_notes,
                    output_file=output_file
                )
                if result:
                    print(f"âœ… æˆåŠŸç”Ÿæˆ {emotion} éŸ³ä¹: {result}")
                else:
                    print(f"âŒ ç”Ÿæˆ {emotion} éŸ³ä¹å¤±è´¥")
        else:  # inpainting
            # ç¤ºä¾‹ï¼šè¡¥å…¨ä¸€ä¸ªè¾“å…¥MIDIï¼ˆéœ€ç”¨æˆ·æŒ‡å®šï¼‰
            input_midi = "input_seed.mid"  # <<< ç”¨æˆ·å¯ä¿®æ”¹æ­¤è·¯å¾„
            if os.path.exists(input_midi):
                print(f"ğŸ¹ å¼€å§‹æ—‹å¾‹è¡¥å…¨ï¼Œè¾“å…¥æ–‡ä»¶: {input_midi}")
                result = generator.complete_melody(
                    input_midi_path=input_midi,
                    output_file="completed_output.mid",
                    num_completion_notes=100
                )
                if result:
                    print(f"âœ… æ—‹å¾‹è¡¥å…¨æˆåŠŸ: {result}")
                else:
                    print("âŒ æ—‹å¾‹è¡¥å…¨å¤±è´¥")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è¾“å…¥MIDIæ–‡ä»¶: {input_midi}")
                print("ğŸ’¡ è¯·åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸€äº›éŸ³ç¬¦çš„ input_seed.mid æ–‡ä»¶ï¼Œæˆ–ä¿®æ”¹ main() ä¸­çš„ input_midi è·¯å¾„")
    else:
        print("ğŸ”„ å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
        model = generator.train_from_scratch()
        if model is not None and generator.mode == "conditional":
            emotions = list(generator.utils.emotion_to_id.keys())
            if emotions:
                first_emotion = emotions[0]
                output_file = os.path.join(output_dir, f"demo_output_{first_emotion}.mid")
                result = generator.generate_conditional(
                    emotion=first_emotion,
                    num_notes=default_num_notes,
                    output_file=output_file
                )
                if result:
                    print(f"âœ… æˆåŠŸç”Ÿæˆæ¼”ç¤ºéŸ³ä¹: {result}")
                else:
                    print("âŒ ç”Ÿæˆæ¼”ç¤ºéŸ³ä¹å¤±è´¥")


if __name__ == "__main__":
    main()